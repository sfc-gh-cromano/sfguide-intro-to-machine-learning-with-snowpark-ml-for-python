{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ML Modeling\n",
    "\n",
    "- In this notebook, we will illustrate how to train an XGBoost model with the diamonds dataset using the Snowpark ML Model API. \n",
    "- We also show how to do inference and deploy the model via Model Registry or as a UDF (See Appendix).\n",
    "\n",
    "The Snowpark ML Model API currently supports sklearn, xgboost, and lightgbm models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowpark for Python\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.version import VERSION\n",
    "from snowflake.snowpark.functions import udf\n",
    "import snowflake.snowpark.functions as F\n",
    "\n",
    "# Snowpark ML\n",
    "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "from snowflake.ml.modeling.model_selection import GridSearchCV\n",
    "from snowflake.ml.registry import model_registry\n",
    "from snowflake.ml._internal.utils import identifier\n",
    "from snowflake.ml.utils import connection_params\n",
    "\n",
    "# data science libs\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from snowflake.ml.modeling.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# misc\n",
    "import json\n",
    "import joblib\n",
    "import cachetools\n",
    "\n",
    "# warning suppresion\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish a Secure Connection to Snowflake\n",
    "\n",
    "*Other connection options include Username/Password, MFA, OAuth, Okta, SSO. For more information, refer to the [Python Connector](https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-example) documentation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Snowpark Connection\n",
    "session = Session.builder.configs(\n",
    "connection_params.SnowflakeLoginOptions()).getOrCreate()\n",
    "session.sql_simplifier_enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the table name where we stored the diamonds dataset\n",
    "# **nChange this only if you named your table something else in the data ingest notebook **\n",
    "DEMO_TABLE = 'diamonds' \n",
    "input_tbl = f\"{session.get_current_database()}.{session.get_current_schema()}.{DEMO_TABLE}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data & preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "diamonds_df = session.table(input_tbl)\n",
    "diamonds_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize all the features for modeling\n",
    "CATEGORICAL_COLUMNS = [\"CUT\", \"COLOR\", \"CLARITY\"]\n",
    "CATEGORICAL_COLUMNS_OE = [\"CUT_OE\", \"COLOR_OE\", \"CLARITY_OE\"] # To name the ordinal encoded columns\n",
    "NUMERICAL_COLUMNS = [\"CARAT\", \"DEPTH\", \"TABLE_PCT\", \"X\", \"Y\", \"Z\"]\n",
    "\n",
    "LABEL_COLUMNS = ['PRICE']\n",
    "OUTPUT_COLUMNS = ['PREDICTED_PRICE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessing pipeline object (locally in this case but you can also load the one saved to stage in the previous notebook as an optional exercise)\n",
    "PIPELINE_FILE = 'preprocessing_pipeline.joblib'\n",
    "preprocessing_pipeline = joblib.load(PIPELINE_FILE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a simple XGBoost Regression model\n",
    "\n",
    "What's happening here? \n",
    "\n",
    "- The `model.fit()` function actually creates a temporary stored procedure in the background. This also means that the model training is a single-node operation. Be sure to use a [Snowpark Optimized Warehouse](https://docs.snowflake.com/en/user-guide/warehouses-snowpark-optimized) if you need more memory. We are just using an XS Standard Virtual Warehouse here, which we created at the beginning of this quickstart.\n",
    "- The `model.predict()` function actualls creates a temporary vectorized UDF in the background, which means the input DataFrame is batched as Pandas DataFrames and inference is parallelized across the batches of data.\n",
    "\n",
    "You can check the query history once you execute the following cell to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "diamonds_train_df, diamonds_test_df = diamonds_df.random_split(weights=[0.9, 0.1], seed=0)\n",
    "\n",
    "# Run the train and test sets through the Pipeline object we defined earlier\n",
    "train_df = preprocessing_pipeline.fit(diamonds_train_df).transform(diamonds_train_df)\n",
    "test_df = preprocessing_pipeline.transform(diamonds_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGBRegressor\n",
    "regressor = XGBRegressor(\n",
    "    input_cols=CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS,\n",
    "    label_cols=LABEL_COLUMNS,\n",
    "    output_cols=OUTPUT_COLUMNS\n",
    ")\n",
    "\n",
    "# Train\n",
    "regressor.fit(train_df)\n",
    "\n",
    "# Predict\n",
    "result = regressor.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to illustrate, we can also pass in a Pandas DataFrame to Snowpark ML's model.predict()\n",
    "regressor.predict(test_df[CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS].to_pandas())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the results using Snowpark ML's MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape = mean_absolute_percentage_error(df=result, \n",
    "                                        y_true_col_names=\"PRICE\", \n",
    "                                        y_pred_col_names=\"PREDICTED_PRICE\")\n",
    "\n",
    "result.select(\"PRICE\", \"PREDICTED_PRICE\").show()\n",
    "print(f\"Mean absolute percentage error: {mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted \n",
    "g = sns.relplot(data=result[\"PRICE\", \"PREDICTED_PRICE\"].to_pandas().astype(\"float64\"), x=\"PRICE\", y=\"PREDICTED_PRICE\", kind=\"scatter\")\n",
    "g.ax.axline((0,0), slope=1, color=\"r\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's use Snowpark ML's **Distributed** `GridSearchCV()` function to find optimal model parameters\n",
    "\n",
    "We will increase the warehouse size to scale up our hyperparameter tuning to take advantage of parallelized model training to accelerate this search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"ALTER WAREHOUSE ML_HOL_WH SET WAREHOUSE_SIZE=LARGE;\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    estimator=XGBRegressor(),\n",
    "    param_grid={\n",
    "        \"n_estimators\":[100, 200, 300, 400, 500],\n",
    "        \"learning_rate\":[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    },\n",
    "    n_jobs = -1,\n",
    "    scoring=\"neg_mean_absolute_percentage_error\",\n",
    "    input_cols=CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS,\n",
    "    label_cols=LABEL_COLUMNS,\n",
    "    output_cols=OUTPUT_COLUMNS\n",
    ")\n",
    "\n",
    "# Train\n",
    "grid_search.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"ALTER WAREHOUSE ML_HOL_WH SET WAREHOUSE_SIZE=XSMALL;\").collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the best estimator has the following parameters: `n_estimators=500` & `learning_rate=0.4`.\n",
    "\n",
    "We can use `to_sklearn()` in order to get the actual xgboost model object, which gives us access to all its attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.to_sklearn().best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also analyze the grid search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze grid search results\n",
    "gs_results = grid_search.to_sklearn().cv_results_\n",
    "n_estimators_val = []\n",
    "learning_rate_val = []\n",
    "for param_dict in gs_results[\"params\"]:\n",
    "    n_estimators_val.append(param_dict[\"n_estimators\"])\n",
    "    learning_rate_val.append(param_dict[\"learning_rate\"])\n",
    "mape_val = gs_results[\"mean_test_score\"]*-1\n",
    "\n",
    "gs_results_df = pd.DataFrame(data={\n",
    "    \"n_estimators\":n_estimators_val,\n",
    "    \"learning_rate\":learning_rate_val,\n",
    "    \"mape\":mape_val})\n",
    "\n",
    "sns.relplot(data=gs_results_df, x=\"learning_rate\", y=\"mape\", hue=\"n_estimators\", kind=\"line\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is consistent with the `learning_rate=0.4` and `n_estimator=500` chosen as the best estimator with the lowest MAPE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's predict and analyze the results from using the best estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "result = grid_search.predict(test_df)\n",
    "\n",
    "# Analyze results\n",
    "mape = mean_absolute_percentage_error(df=result, \n",
    "                                        y_true_col_names=\"PRICE\", \n",
    "                                        y_pred_col_names=\"PREDICTED_PRICE\")\n",
    "\n",
    "result.select(\"PRICE\", \"PREDICTED_PRICE\").show()\n",
    "print(f\"Mean absolute percentage error: {mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted \n",
    "g = sns.relplot(data=result[\"PRICE\", \"PREDICTED_PRICE\"].to_pandas().astype(\"float64\"), x=\"PRICE\", y=\"PREDICTED_PRICE\", kind=\"scatter\")\n",
    "g.ax.axline((0,0), slope=1, color=\"r\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model deployment using Model Registry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with Snowpark ML's model registry, we have a Snowflake native model versioning and deployment framework. This allows us to log models, tag parameters and metrics, track metadata, create versions, and ultimately deploy models into a Snowflake warehouse or Snowpark Container Service for batch scoring tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save our optimal model first and its metadata\n",
    "optimal_model = grid_search.to_sklearn().best_estimator_\n",
    "optimal_n_estimators = grid_search.to_sklearn().best_estimator_.n_estimators\n",
    "optimal_learning_rate = grid_search.to_sklearn().best_estimator_.learning_rate\n",
    "\n",
    "optimal_mape = gs_results_df.loc[(gs_results_df['n_estimators']==optimal_n_estimators) &\n",
    "                                 (gs_results_df['learning_rate']==optimal_learning_rate), 'mape'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will log our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample input data to pass into the registry logging function\n",
    "X = train_df.select(CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS).limit(100)\n",
    "\n",
    "db = identifier._get_unescaped_name(session.get_current_database())\n",
    "schema = identifier._get_unescaped_name(session.get_current_schema())\n",
    "\n",
    "# Define model name and version\n",
    "model_name = \"diamonds_model\"\n",
    "model_version = 1\n",
    "\n",
    "# Create a registry and log the model\n",
    "registry = model_registry.ModelRegistry(session=session, database_name=db, schema_name=schema, create_if_not_exists=True)\n",
    "\n",
    "registry.log_model(\n",
    "    model_name=model_name,\n",
    "    model_version=model_version,\n",
    "    model=optimal_model,\n",
    "    sample_input_data=X,\n",
    "    options={\"embed_local_ml_library\": True, # This option is enabled to pull latest dev code changes.\n",
    "             \"relax\": True} # relax dependencies\n",
    ")\n",
    "\n",
    "# Add evaluation metric\n",
    "registry.set_metric(model_name=model_name, model_version=model_version, metric_name=\"mean_abs_pct_err\", metric_value=optimal_mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's confirm it was added\n",
    "registry.list_models().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to deploy to a Snowflake Warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a deployment name and deploy\n",
    "model_deployment_name = model_name + f\"{model_version}\" + \"_UDF\"\n",
    "\n",
    "registry.deploy(model_name=model_name,\n",
    "                model_version=model_version,\n",
    "                deployment_name=model_deployment_name, \n",
    "                target_method=\"predict\", \n",
    "                permanent=True, \n",
    "                options={\"relax_version\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's confirm it was added\n",
    "registry.list_deployments(model_name, model_version).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the deployed model to perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can always get a reference to our registry using this function call\n",
    "model_ref = model_registry.ModelReference(registry=registry, model_name=model_name, model_version=model_version)\n",
    "\n",
    "# We can then use the deployed model to perform inference\n",
    "result_sdf = model_ref.predict(deployment_name=model_deployment_name, data=test_df)\n",
    "#result_sdf.rename(F.col('\"output_feature_0\"'),\"PREDICTED_PRICE\").show()\n",
    "result_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref.deploy(deployment_name=\"Diamonds_predict\",\n",
    "             target_method=\"predict\",    # the name of the model's method, usually predict\n",
    "             permanent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref.predict(\"Diamonds_predict\", test_df).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.write.mode(\"overwrite\").save_as_table(\n",
    "    \"TEST_DATA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some clean up now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "registry.delete_deployment(model_name=model_name, model_version=model_version, deployment_name=model_deployment_name)\n",
    "registry.delete_model(model_name=model_name, model_version=model_version, delete_artifact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry.list_deployments(model_name, model_version).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry.list_models().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy model using a Vectorized UDF\n",
    "\n",
    "In case you need to deploy a model not trained using Snowpark ML or externally trained, you can still deploy via Vectorized UDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save our optimal model first\n",
    "MODEL_FILE = 'model.joblib'\n",
    "joblib.dump(optimal_model, MODEL_FILE) # we are just pickling it locally first\n",
    "\n",
    "# You can also save the pickled object into the stage we created earlier\n",
    "session.file.put(MODEL_FILE, \"@ML_HOL_ASSETS\", overwrite=True)\n",
    "\n",
    "# Get all relevant column names to pass into the UDF call\n",
    "feature_cols = test_df[CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache the model load to optimize inference\n",
    "@cachetools.cached(cache={})\n",
    "def load_model(filename):\n",
    "    import joblib\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "    import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "\n",
    "    if import_dir:\n",
    "        with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "            m = joblib.load(file)\n",
    "            return m\n",
    "\n",
    "# Register the UDF via decorator\n",
    "@udf(name='batch_predict_diamond', \n",
    "     session=session, \n",
    "     replace=True, \n",
    "     is_permanent=True, \n",
    "     stage_location='@ML_HOL_ASSETS',\n",
    "     input_types=[F.FloatType()]*len(feature_cols),\n",
    "     return_type=F.FloatType(),\n",
    "     imports=['@ML_HOL_ASSETS/model.joblib.gz'],\n",
    "     packages=['pandas','joblib','cachetools','xgboost'])\n",
    "def batch_predict_diamond(test_df: pd.DataFrame) -> pd.Series:\n",
    "    # Need to name the columns because column names aren't passed in to this function\n",
    "    test_df.columns = [\"CUT_OE\", \"COLOR_OE\", \"CLARITY_OE\", 'CARAT', 'DEPTH', 'TABLE_PCT', 'X', 'Y', 'Z']\n",
    "    model = load_model('model.joblib.gz')\n",
    "    return model.predict(test_df) # This is using the XGBoost library's model.predict(), not Snowpark ML's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call Vectorized User-Defined Function (UDF) on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_w_preds = test_df.with_column('PREDICTED_PRICE', batch_predict_diamond(*feature_cols))\n",
    "test_df_w_preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark-ml-hol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
